{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e181f48",
   "metadata": {},
   "source": [
    "# Saudi Arabia Dataset Analysis with Cross-Dataset Validation\n",
    "\n",
    "This notebook performs comprehensive machine learning analysis on the Saudi Arabia autism screening dataset and validates the best model on:\n",
    "1. **Polish Dataset** - selecting 10 out of 25 Q-CHAT questions (group labels: 1/7 for ASD)\n",
    "2. **Bangladesh Dataset** - using the existing SVM child model for validation\n",
    "\n",
    "## Objectives\n",
    "- Train multiple ML models on Saudi Arabia toddler dataset\n",
    "- Identify the best performing model\n",
    "- Validate best model on Polish dataset with feature mapping\n",
    "- Validate SVM child model on Bangladesh/Real World Child dataset\n",
    "- Compare cross-dataset performance and generalization capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b511a57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n",
      "Working directory: e:\\Users\\Prajj\\Documents\\7th Sem\\RM\\Codes\n",
      "Current time: 2025-09-23 17:51:42.946675\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score, \n",
    "                             precision_score, recall_score, roc_auc_score, roc_curve,\n",
    "                             f1_score)\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "import warnings\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure settings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "plt.style.use('default')\n",
    "\n",
    "# Set display options for pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "print(f\"Current time: {pd.Timestamp.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcfbb11",
   "metadata": {},
   "source": [
    "## Load and Explore Saudi Arabia Dataset\n",
    "\n",
    "Load the Saudi Arabia toddler autism screening dataset and examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4819a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saudi Arabia dataset loaded successfully!\n",
      "Dataset shape: (506, 17)\n",
      "\n",
      "================================================================================\n",
      "SAUDI ARABIA DATASET OVERVIEW\n",
      "================================================================================\n",
      "\n",
      "Dataset dimensions: 506 rows √ó 17 columns\n",
      "\n",
      "Column names:\n",
      " 1. A10\n",
      " 2. A9\n",
      " 3. A8\n",
      " 4. A7\n",
      " 5. A6\n",
      " 6. A5\n",
      " 7. A4\n",
      " 8. A3\n",
      " 9. A2\n",
      "10. A1\n",
      "11. Region\n",
      "12. Family member with ASD history\n",
      "13. Who is completing the test\n",
      "14. Age\n",
      "15. Gender\n",
      "16. Screening Score\n",
      "17. Class\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A10</th>\n",
       "      <th>A9</th>\n",
       "      <th>A8</th>\n",
       "      <th>A7</th>\n",
       "      <th>A6</th>\n",
       "      <th>A5</th>\n",
       "      <th>A4</th>\n",
       "      <th>A3</th>\n",
       "      <th>A2</th>\n",
       "      <th>A1</th>\n",
       "      <th>Region</th>\n",
       "      <th>Family member with ASD history</th>\n",
       "      <th>Who is completing the test</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Screening Score</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Makkah Province</td>\n",
       "      <td>No</td>\n",
       "      <td>Family member</td>\n",
       "      <td>32</td>\n",
       "      <td>Female</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Makkah Province</td>\n",
       "      <td>No</td>\n",
       "      <td>Family member</td>\n",
       "      <td>30</td>\n",
       "      <td>Female</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Makkah Province</td>\n",
       "      <td>No</td>\n",
       "      <td>Family member</td>\n",
       "      <td>36</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Makkah Province</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Family member</td>\n",
       "      <td>36</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Eastern Province</td>\n",
       "      <td>No</td>\n",
       "      <td>Family member</td>\n",
       "      <td>36</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A10  A9  A8  A7  A6  A5  A4  A3  A2  A1            Region  \\\n",
       "0    0   0   1   1   1   0   0   0   0   0   Makkah Province   \n",
       "1    0   0   1   0   0   1   0   1   0   0   Makkah Province   \n",
       "2    0   0   0   1   0   0   0   0   0   0   Makkah Province   \n",
       "3    0   0   0   0   0   0   0   0   0   0   Makkah Province   \n",
       "4    0   0   0   0   0   0   0   0   0   0  Eastern Province   \n",
       "\n",
       "  Family member with ASD history Who is completing the test  Age  Gender  \\\n",
       "0                             No              Family member   32  Female   \n",
       "1                             No              Family member   30  Female   \n",
       "2                             No              Family member   36    Male   \n",
       "3                            Yes              Family member   36  Female   \n",
       "4                             No              Family member   36  Female   \n",
       "\n",
       "   Screening Score  Class  \n",
       "0                3      0  \n",
       "1                3      0  \n",
       "2                1      0  \n",
       "3                0      0  \n",
       "4                0      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 17 columns):\n",
      " #   Column                          Non-Null Count  Dtype \n",
      "---  ------                          --------------  ----- \n",
      " 0   A10                             506 non-null    int64 \n",
      " 1   A9                              506 non-null    int64 \n",
      " 2   A8                              506 non-null    int64 \n",
      " 3   A7                              506 non-null    int64 \n",
      " 4   A6                              506 non-null    int64 \n",
      " 5   A5                              506 non-null    int64 \n",
      " 6   A4                              506 non-null    int64 \n",
      " 7   A3                              506 non-null    int64 \n",
      " 8   A2                              506 non-null    int64 \n",
      " 9   A1                              506 non-null    int64 \n",
      " 10  Region                          506 non-null    object\n",
      " 11  Family member with ASD history  506 non-null    object\n",
      " 12  Who is completing the test      506 non-null    object\n",
      " 13  Age                             506 non-null    int64 \n",
      " 14  Gender                          506 non-null    object\n",
      " 15  Screening Score                 506 non-null    int64 \n",
      " 16  Class                           506 non-null    int64 \n",
      "dtypes: int64(13), object(4)\n",
      "memory usage: 67.3+ KB\n",
      "None\n",
      "\n",
      "Basic statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A10</th>\n",
       "      <th>A9</th>\n",
       "      <th>A8</th>\n",
       "      <th>A7</th>\n",
       "      <th>A6</th>\n",
       "      <th>A5</th>\n",
       "      <th>A4</th>\n",
       "      <th>A3</th>\n",
       "      <th>A2</th>\n",
       "      <th>A1</th>\n",
       "      <th>Age</th>\n",
       "      <th>Screening Score</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.620553</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.575099</td>\n",
       "      <td>0.553360</td>\n",
       "      <td>0.549407</td>\n",
       "      <td>0.559289</td>\n",
       "      <td>0.525692</td>\n",
       "      <td>0.511858</td>\n",
       "      <td>0.488142</td>\n",
       "      <td>0.563241</td>\n",
       "      <td>24.448617</td>\n",
       "      <td>5.490119</td>\n",
       "      <td>0.673913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.485730</td>\n",
       "      <td>0.498599</td>\n",
       "      <td>0.494817</td>\n",
       "      <td>0.497637</td>\n",
       "      <td>0.498045</td>\n",
       "      <td>0.496964</td>\n",
       "      <td>0.499834</td>\n",
       "      <td>0.500354</td>\n",
       "      <td>0.500354</td>\n",
       "      <td>0.496475</td>\n",
       "      <td>8.344461</td>\n",
       "      <td>3.181771</td>\n",
       "      <td>0.469243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              A10          A9          A8          A7          A6          A5  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     0.620553    0.543478    0.575099    0.553360    0.549407    0.559289   \n",
       "std      0.485730    0.498599    0.494817    0.497637    0.498045    0.496964   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "75%      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               A4          A3          A2          A1         Age  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     0.525692    0.511858    0.488142    0.563241   24.448617   \n",
       "std      0.499834    0.500354    0.500354    0.496475    8.344461   \n",
       "min      0.000000    0.000000    0.000000    0.000000   12.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000   17.000000   \n",
       "50%      1.000000    1.000000    0.000000    1.000000   24.000000   \n",
       "75%      1.000000    1.000000    1.000000    1.000000   33.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000   36.000000   \n",
       "\n",
       "       Screening Score       Class  \n",
       "count       506.000000  506.000000  \n",
       "mean          5.490119    0.673913  \n",
       "std           3.181771    0.469243  \n",
       "min           0.000000    0.000000  \n",
       "25%           3.000000    0.000000  \n",
       "50%           6.000000    1.000000  \n",
       "75%           8.000000    1.000000  \n",
       "max          10.000000    1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values:\n",
      "No missing values found!\n",
      "\n",
      "Target variable 'Family member with ASD history' distribution:\n",
      "Family member with ASD history\n",
      "No     384\n",
      "Yes    122\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Target variable percentages:\n",
      "No: 75.89%\n",
      "Yes: 24.11%\n",
      "\n",
      "Identified feature columns (10):\n",
      "  - A10\n",
      "    Distribution: {1: 314, 0: 192}\n",
      "  - A9\n",
      "    Distribution: {1: 275, 0: 231}\n",
      "  - A8\n",
      "    Distribution: {1: 291, 0: 215}\n",
      "  - A7\n",
      "    Distribution: {1: 280, 0: 226}\n",
      "  - A6\n",
      "    Distribution: {1: 278, 0: 228}\n",
      "  - A5\n",
      "    Distribution: {1: 283, 0: 223}\n",
      "  - A4\n",
      "    Distribution: {1: 266, 0: 240}\n",
      "  - A3\n",
      "    Distribution: {1: 259, 0: 247}\n",
      "  - A2\n",
      "    Distribution: {0: 259, 1: 247}\n",
      "  - A1\n",
      "    Distribution: {1: 285, 0: 221}\n"
     ]
    }
   ],
   "source": [
    "# Load Saudi Arabia Dataset\n",
    "saudi_dataset_path = r'e:\\Users\\Prajj\\Documents\\7th Sem\\RM\\Datasets\\Saudi Dataset Toddlers\\Autism Spectrum Disorder Screening Data for Toddlers in Saudi Arabia Data Set.csv'\n",
    "\n",
    "try:\n",
    "    # Load the dataset\n",
    "    df_saudi = pd.read_csv(saudi_dataset_path)\n",
    "    print(\"‚úÖ Saudi Arabia dataset loaded successfully!\")\n",
    "    print(f\"Dataset shape: {df_saudi.shape}\")\n",
    "    \n",
    "    # Display basic information\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAUDI ARABIA DATASET OVERVIEW\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nDataset dimensions: {df_saudi.shape[0]} rows √ó {df_saudi.shape[1]} columns\")\n",
    "    \n",
    "    print(\"\\nColumn names:\")\n",
    "    for i, col in enumerate(df_saudi.columns, 1):\n",
    "        print(f\"{i:2d}. {col}\")\n",
    "    \n",
    "    print(\"\\nFirst few rows:\")\n",
    "    display(df_saudi.head())\n",
    "    \n",
    "    print(\"\\nDataset info:\")\n",
    "    print(df_saudi.info())\n",
    "    \n",
    "    print(\"\\nBasic statistics:\")\n",
    "    display(df_saudi.describe())\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(\"\\nMissing values:\")\n",
    "    missing_values = df_saudi.isnull().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(missing_values[missing_values > 0])\n",
    "    else:\n",
    "        print(\"No missing values found!\")\n",
    "    \n",
    "    # Analyze target variable\n",
    "    target_candidates = ['Class/ASD', 'ASD', 'autism', 'classification', 'target']\n",
    "    target_col = None\n",
    "    \n",
    "    for col in df_saudi.columns:\n",
    "        if any(candidate.lower() in col.lower() for candidate in target_candidates):\n",
    "            target_col = col\n",
    "            break\n",
    "    \n",
    "    if target_col:\n",
    "        print(f\"\\nTarget variable '{target_col}' distribution:\")\n",
    "        target_counts = df_saudi[target_col].value_counts()\n",
    "        print(target_counts)\n",
    "        \n",
    "        print(f\"\\nTarget variable percentages:\")\n",
    "        target_percentages = df_saudi[target_col].value_counts(normalize=True) * 100\n",
    "        for value, percentage in target_percentages.items():\n",
    "            print(f\"{value}: {percentage:.2f}%\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Warning: Target column not clearly identified\")\n",
    "        print(\"Last few columns (potential targets):\", list(df_saudi.columns[-3:]))\n",
    "    \n",
    "    # Identify feature columns (A1-A10 scores)\n",
    "    feature_cols = [col for col in df_saudi.columns if 'A' in col and 'Score' in col]\n",
    "    if not feature_cols:\n",
    "        # Alternative patterns\n",
    "        feature_cols = [col for col in df_saudi.columns if col.startswith('A') and any(char.isdigit() for char in col)]\n",
    "    \n",
    "    print(f\"\\nIdentified feature columns ({len(feature_cols)}):\")\n",
    "    for col in feature_cols:\n",
    "        print(f\"  - {col}\")\n",
    "        if len(feature_cols) <= 10:  # Show distribution for small number of features\n",
    "            print(f\"    Distribution: {df_saudi[col].value_counts().to_dict()}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: Dataset file not found at {saudi_dataset_path}\")\n",
    "    print(\"Please check the file path.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07ad743c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PREPROCESSING SAUDI ARABIA DATASET\n",
      "============================================================\n",
      "Original dataset shape: (506, 17)\n",
      "\n",
      "Handling missing values...\n",
      "Missing values before preprocessing: 0\n",
      "  Encoded Region: 13 unique values\n",
      "  Encoded Family member with ASD history: 2 unique values\n",
      "  Encoded Who is completing the test: 2 unique values\n",
      "  Encoded Gender: 2 unique values\n",
      "\n",
      "Target variable preprocessing...\n",
      "Original target 'Family member with ASD history' values: [np.int64(0), np.int64(1)]\n",
      "Encoded target values: [np.int64(0), np.int64(1)]\n",
      "Mapping: {np.int64(0): np.int64(0), np.int64(1): np.int64(1)}\n",
      "Missing values after preprocessing: 0\n",
      "Processed dataset shape: (506, 17)\n",
      "\n",
      "============================================================\n",
      "PROCESSED SAUDI DATASET SUMMARY\n",
      "============================================================\n",
      "\n",
      "Processed dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 17 columns):\n",
      " #   Column                          Non-Null Count  Dtype\n",
      "---  ------                          --------------  -----\n",
      " 0   A10                             506 non-null    int64\n",
      " 1   A9                              506 non-null    int64\n",
      " 2   A8                              506 non-null    int64\n",
      " 3   A7                              506 non-null    int64\n",
      " 4   A6                              506 non-null    int64\n",
      " 5   A5                              506 non-null    int64\n",
      " 6   A4                              506 non-null    int64\n",
      " 7   A3                              506 non-null    int64\n",
      " 8   A2                              506 non-null    int64\n",
      " 9   A1                              506 non-null    int64\n",
      " 10  Region                          506 non-null    int64\n",
      " 11  Family member with ASD history  506 non-null    int64\n",
      " 12  Who is completing the test      506 non-null    int64\n",
      " 13  Age                             506 non-null    int64\n",
      " 14  Gender                          506 non-null    int64\n",
      " 15  Screening Score                 506 non-null    int64\n",
      " 16  Class                           506 non-null    int64\n",
      "dtypes: int64(17)\n",
      "memory usage: 67.3 KB\n",
      "None\n",
      "\n",
      "Target distribution after preprocessing:\n",
      "Family member with ASD history\n",
      "0    384\n",
      "1    122\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Preprocess Saudi Arabia Dataset\n",
    "def preprocess_saudi_dataset(df):\n",
    "    \"\"\"\n",
    "    Comprehensive preprocessing for the Saudi Arabia autism dataset\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"PREPROCESSING SAUDI ARABIA DATASET\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    print(f\"Original dataset shape: {df_processed.shape}\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(f\"\\nHandling missing values...\")\n",
    "    missing_before = df_processed.isnull().sum().sum()\n",
    "    print(f\"Missing values before preprocessing: {missing_before}\")\n",
    "    \n",
    "    # Fill missing values for numerical columns\n",
    "    numerical_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    for col in numerical_cols:\n",
    "        if df_processed[col].isnull().sum() > 0:\n",
    "            if 'age' in col.lower():\n",
    "                # For age, use median\n",
    "                median_value = df_processed[col].median()\n",
    "                df_processed[col].fillna(median_value, inplace=True)\n",
    "                print(f\"  Filled {col} missing values with median: {median_value}\")\n",
    "            else:\n",
    "                # For other numerical columns, use mode or median\n",
    "                mode_val = df_processed[col].mode()\n",
    "                if len(mode_val) > 0:\n",
    "                    df_processed[col].fillna(mode_val[0], inplace=True)\n",
    "                    print(f\"  Filled {col} missing values with mode: {mode_val[0]}\")\n",
    "    \n",
    "    # Fill missing values for categorical columns\n",
    "    categorical_cols = df_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "    for col in categorical_cols:\n",
    "        if df_processed[col].isnull().sum() > 0:\n",
    "            mode_value = df_processed[col].mode()[0] if not df_processed[col].mode().empty else 'Unknown'\n",
    "            df_processed[col].fillna(mode_value, inplace=True)\n",
    "            print(f\"  Filled {col} missing values with mode: {mode_value}\")\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        if col not in ['Class/ASD', 'ASD']:  # Don't encode target yet\n",
    "            le = LabelEncoder()\n",
    "            df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "            print(f\"  Encoded {col}: {len(le.classes_)} unique values\")\n",
    "    \n",
    "    # Handle target variable\n",
    "    target_col = None\n",
    "    for col in df_processed.columns:\n",
    "        if any(target.lower() in col.lower() for target in ['class', 'asd', 'autism']):\n",
    "            target_col = col\n",
    "            break\n",
    "    \n",
    "    if target_col:\n",
    "        print(f\"\\nTarget variable preprocessing...\")\n",
    "        print(f\"Original target '{target_col}' values: {sorted(df_processed[target_col].unique())}\")\n",
    "        \n",
    "        # Encode target variable\n",
    "        target_encoder = LabelEncoder()\n",
    "        df_processed[target_col] = target_encoder.fit_transform(df_processed[target_col])\n",
    "        print(f\"Encoded target values: {sorted(df_processed[target_col].unique())}\")\n",
    "        print(f\"Mapping: {dict(zip(target_encoder.classes_, target_encoder.transform(target_encoder.classes_)))}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Warning: Target column not found!\")\n",
    "        target_encoder = None\n",
    "        # Assume last column is target\n",
    "        target_col = df_processed.columns[-1]\n",
    "        print(f\"Using last column as target: {target_col}\")\n",
    "    \n",
    "    missing_after = df_processed.isnull().sum().sum()\n",
    "    print(f\"Missing values after preprocessing: {missing_after}\")\n",
    "    print(f\"Processed dataset shape: {df_processed.shape}\")\n",
    "    \n",
    "    return df_processed, label_encoders, target_encoder, target_col\n",
    "\n",
    "# Apply preprocessing\n",
    "if 'df_saudi' in locals():\n",
    "    df_saudi_processed, saudi_label_encoders, saudi_target_encoder, saudi_target_col = preprocess_saudi_dataset(df_saudi)\n",
    "    \n",
    "    # Display processed dataset info\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"PROCESSED SAUDI DATASET SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nProcessed dataset info:\")\n",
    "    print(df_saudi_processed.info())\n",
    "    \n",
    "    print(f\"\\nTarget distribution after preprocessing:\")\n",
    "    if saudi_target_col:\n",
    "        target_dist = df_saudi_processed[saudi_target_col].value_counts()\n",
    "        print(target_dist)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please run the data loading cell first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a157f0b6",
   "metadata": {},
   "source": [
    "## Train Multiple Models on Saudi Arabia Dataset\n",
    "\n",
    "Train various machine learning models on the Saudi Arabia dataset to find the best performing classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98089383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MACHINE LEARNING MODELS CONFIGURATION\n",
      "============================================================\n",
      "Total models to train: 8\n",
      "\n",
      "Models configured:\n",
      " 1. SVM (RBF)\n",
      " 2. SVM (Linear)\n",
      " 3. Random Forest\n",
      " 4. Logistic Regression\n",
      " 5. Gradient Boosting\n",
      " 6. Naive Bayes\n",
      " 7. K-Nearest Neighbors\n",
      " 8. Decision Tree\n",
      "\n",
      "‚úÖ Models and evaluation functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define machine learning models for Saudi Arabia dataset\n",
    "models_dict = {\n",
    "    'SVM (RBF)': SVC(kernel='rbf', random_state=42, probability=True, C=1.0),\n",
    "    'SVM (Linear)': SVC(kernel='linear', random_state=42, probability=True, C=1.0),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10)\n",
    "}\n",
    "\n",
    "# Utility functions for model evaluation\n",
    "def calculate_specificity(y_true, y_pred):\n",
    "    \"\"\"Calculate specificity (True Negative Rate)\"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "\n",
    "def evaluate_model_comprehensive(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with all metrics\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test, y_test_pred, average='binary', zero_division=0)\n",
    "    recall = recall_score(y_test, y_test_pred, average='binary', zero_division=0)\n",
    "    specificity = calculate_specificity(y_test, y_test_pred)\n",
    "    f1 = f1_score(y_test, y_test_pred, average='binary', zero_division=0)\n",
    "    \n",
    "    # Calculate AUC if possible\n",
    "    try:\n",
    "        y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "        auc = roc_auc_score(y_test, y_test_proba)\n",
    "    except:\n",
    "        auc = 0.0\n",
    "    \n",
    "    # Cross-validation\n",
    "    try:\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "        cv_mean = cv_scores.mean()\n",
    "        cv_std = cv_scores.std()\n",
    "    except:\n",
    "        cv_mean = 0.0\n",
    "        cv_std = 0.0\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Train_Accuracy': train_accuracy,\n",
    "        'Test_Accuracy': test_accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'Specificity': specificity,\n",
    "        'F1_Score': f1,\n",
    "        'AUC': auc,\n",
    "        'CV_Mean': cv_mean,\n",
    "        'CV_Std': cv_std,\n",
    "        'Training_Time': training_time\n",
    "    }\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MACHINE LEARNING MODELS CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Total models to train: {len(models_dict)}\")\n",
    "print(\"\\nModels configured:\")\n",
    "for i, (name, model) in enumerate(models_dict.items(), 1):\n",
    "    print(f\"{i:2d}. {name}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Models and evaluation functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0beb0a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREPARING SAUDI ARABIA DATASET FOR TRAINING\n",
      "================================================================================\n",
      "Total features available: 16\n",
      "Target column: Family member with ASD history\n",
      "Feature columns: ['A10', 'A9', 'A8', 'A7', 'A6', 'A5', 'A4', 'A3', 'A2', 'A1', 'Region', 'Who is completing the test', 'Age', 'Gender', 'Screening Score', 'Class']\n",
      "\n",
      "Dataset preparation:\n",
      "  Feature matrix shape: (506, 16)\n",
      "  Target vector shape: (506,)\n",
      "  Target distribution: [384 122]\n",
      "\n",
      "Data split:\n",
      "  Training set: 404 samples\n",
      "  Test set: 102 samples\n",
      "  Training target distribution: [307  97]\n",
      "  Test target distribution: [77 25]\n",
      "\n",
      "Feature scaling completed:\n",
      "  Training features shape: (404, 16)\n",
      "  Test features shape: (102, 16)\n",
      "\n",
      "‚úÖ Saudi Arabia dataset preparation completed!\n"
     ]
    }
   ],
   "source": [
    "# Prepare Saudi Arabia dataset for training\n",
    "if 'df_saudi_processed' in locals() and saudi_target_col:\n",
    "    print(\"=\"*80)\n",
    "    print(\"PREPARING SAUDI ARABIA DATASET FOR TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Identify feature columns\n",
    "    exclude_cols = [saudi_target_col, 'Case_No', 'ID', 'case_no', 'id']\n",
    "    feature_columns = [col for col in df_saudi_processed.columns \n",
    "                      if col not in exclude_cols and not any(ex.lower() in col.lower() for ex in exclude_cols)]\n",
    "    \n",
    "    print(f\"Total features available: {len(feature_columns)}\")\n",
    "    print(f\"Target column: {saudi_target_col}\")\n",
    "    print(f\"Feature columns: {feature_columns}\")\n",
    "    \n",
    "    # Prepare X (features) and y (target)\n",
    "    X_saudi = df_saudi_processed[feature_columns]\n",
    "    y_saudi = df_saudi_processed[saudi_target_col]\n",
    "    \n",
    "    print(f\"\\nDataset preparation:\")\n",
    "    print(f\"  Feature matrix shape: {X_saudi.shape}\")\n",
    "    print(f\"  Target vector shape: {y_saudi.shape}\")\n",
    "    print(f\"  Target distribution: {np.bincount(y_saudi)}\")\n",
    "    \n",
    "    # Split the data\n",
    "    X_train_saudi, X_test_saudi, y_train_saudi, y_test_saudi = train_test_split(\n",
    "        X_saudi, y_saudi, test_size=0.2, random_state=42, stratify=y_saudi\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nData split:\")\n",
    "    print(f\"  Training set: {X_train_saudi.shape[0]} samples\")\n",
    "    print(f\"  Test set: {X_test_saudi.shape[0]} samples\")\n",
    "    print(f\"  Training target distribution: {np.bincount(y_train_saudi)}\")\n",
    "    print(f\"  Test target distribution: {np.bincount(y_test_saudi)}\")\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_saudi = StandardScaler()\n",
    "    X_train_saudi_scaled = scaler_saudi.fit_transform(X_train_saudi)\n",
    "    X_test_saudi_scaled = scaler_saudi.transform(X_test_saudi)\n",
    "    \n",
    "    print(f\"\\nFeature scaling completed:\")\n",
    "    print(f\"  Training features shape: {X_train_saudi_scaled.shape}\")\n",
    "    print(f\"  Test features shape: {X_test_saudi_scaled.shape}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Saudi Arabia dataset preparation completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please run the preprocessing step first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e29e46bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING MODELS ON SAUDI ARABIA DATASET\n",
      "================================================================================\n",
      "Dataset: Saudi Arabia Toddler Autism Screening\n",
      "Training samples: 404\n",
      "Test samples: 102\n",
      "Features: 16\n",
      "\n",
      "Model                Train_Acc  Test_Acc   Precision  Recall     F1       AUC      Time    \n",
      "------------------------------------------------------------------------------------------\n",
      "SVM (RBF)            0.7896    0.7549    0.0000    0.0000    0.0000  0.5455  0.03   s\n",
      "SVM (Linear)         0.7599    0.7549    0.0000    0.0000    0.0000  0.6000  0.01   s\n",
      "Random Forest        0.9653    0.7353    0.4000    0.1600    0.2286  0.6229  0.11   s\n",
      "Logistic Regression  0.7649    0.7157    0.0000    0.0000    0.0000  0.5668  0.01   s\n",
      "Gradient Boosting    0.9059    0.6961    0.2500    0.1200    0.1622  0.5956  0.08   s\n",
      "Naive Bayes          0.6262    0.5686    0.3208    0.6800    0.4359  0.6390  0.00   s\n",
      "K-Nearest Neighbors  0.7921    0.7451    0.4444    0.1600    0.2353  0.5974  0.00   s\n",
      "Decision Tree        0.9505    0.6471    0.3226    0.4000    0.3571  0.5501  0.00   s\n",
      "\n",
      "================================================================================\n",
      "SAUDI ARABIA DATASET - DETAILED RESULTS\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train_Accuracy</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Specificity</th>\n",
       "      <th>F1_Score</th>\n",
       "      <th>AUC</th>\n",
       "      <th>CV_Mean</th>\n",
       "      <th>CV_Std</th>\n",
       "      <th>Training_Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM (RBF)</td>\n",
       "      <td>0.7896</td>\n",
       "      <td>0.7549</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5455</td>\n",
       "      <td>0.7649</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM (Linear)</td>\n",
       "      <td>0.7599</td>\n",
       "      <td>0.7549</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6000</td>\n",
       "      <td>0.7599</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.9653</td>\n",
       "      <td>0.7353</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.9221</td>\n",
       "      <td>0.2286</td>\n",
       "      <td>0.6229</td>\n",
       "      <td>0.7426</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.1123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.7649</td>\n",
       "      <td>0.7157</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.9481</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.5668</td>\n",
       "      <td>0.7599</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.9059</td>\n",
       "      <td>0.6961</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.8831</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.5956</td>\n",
       "      <td>0.6906</td>\n",
       "      <td>0.0377</td>\n",
       "      <td>0.0820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.6262</td>\n",
       "      <td>0.5686</td>\n",
       "      <td>0.3208</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.5325</td>\n",
       "      <td>0.4359</td>\n",
       "      <td>0.6390</td>\n",
       "      <td>0.6238</td>\n",
       "      <td>0.0199</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.7921</td>\n",
       "      <td>0.7451</td>\n",
       "      <td>0.4444</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.9351</td>\n",
       "      <td>0.2353</td>\n",
       "      <td>0.5974</td>\n",
       "      <td>0.7055</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.9505</td>\n",
       "      <td>0.6471</td>\n",
       "      <td>0.3226</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.7273</td>\n",
       "      <td>0.3571</td>\n",
       "      <td>0.5501</td>\n",
       "      <td>0.6733</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.0020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Train_Accuracy  Test_Accuracy  Precision  Recall  \\\n",
       "0            SVM (RBF)          0.7896         0.7549     0.0000    0.00   \n",
       "1         SVM (Linear)          0.7599         0.7549     0.0000    0.00   \n",
       "2        Random Forest          0.9653         0.7353     0.4000    0.16   \n",
       "3  Logistic Regression          0.7649         0.7157     0.0000    0.00   \n",
       "4    Gradient Boosting          0.9059         0.6961     0.2500    0.12   \n",
       "5          Naive Bayes          0.6262         0.5686     0.3208    0.68   \n",
       "6  K-Nearest Neighbors          0.7921         0.7451     0.4444    0.16   \n",
       "7        Decision Tree          0.9505         0.6471     0.3226    0.40   \n",
       "\n",
       "   Specificity  F1_Score     AUC  CV_Mean  CV_Std  Training_Time  \n",
       "0       1.0000    0.0000  0.5455   0.7649  0.0105         0.0255  \n",
       "1       1.0000    0.0000  0.6000   0.7599  0.0057         0.0108  \n",
       "2       0.9221    0.2286  0.6229   0.7426  0.0054         0.1123  \n",
       "3       0.9481    0.0000  0.5668   0.7599  0.0057         0.0144  \n",
       "4       0.8831    0.1622  0.5956   0.6906  0.0377         0.0820  \n",
       "5       0.5325    0.4359  0.6390   0.6238  0.0199         0.0010  \n",
       "6       0.9351    0.2353  0.5974   0.7055  0.0220         0.0010  \n",
       "7       0.7273    0.3571  0.5501   0.6733  0.0183         0.0020  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üèÜ BEST MODEL FOR SAUDI ARABIA DATASET:\n",
      "   Model: SVM (RBF)\n",
      "   Test Accuracy: 0.7549\n",
      "   Precision: 0.0000\n",
      "   Recall: 0.0000\n",
      "   F1-Score: 0.0000\n",
      "   AUC: 0.5455\n",
      "   Cross-Val Accuracy: 0.7649 ¬± 0.0105\n",
      "\n",
      "‚úÖ Best model saved to: e:\\Users\\Prajj\\Documents\\7th Sem\\RM\\Codes\\best_saudi_model.pkl\n",
      "‚úÖ Scaler saved to: e:\\Users\\Prajj\\Documents\\7th Sem\\RM\\Codes\\saudi_scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# Train all models on Saudi Arabia dataset\n",
    "if 'X_train_saudi_scaled' in locals():\n",
    "    print(\"=\"*80)\n",
    "    print(\"TRAINING MODELS ON SAUDI ARABIA DATASET\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    saudi_results = []\n",
    "    trained_saudi_models = {}\n",
    "    \n",
    "    print(f\"Dataset: Saudi Arabia Toddler Autism Screening\")\n",
    "    print(f\"Training samples: {len(X_train_saudi_scaled)}\")\n",
    "    print(f\"Test samples: {len(X_test_saudi_scaled)}\")\n",
    "    print(f\"Features: {X_train_saudi_scaled.shape[1]}\")\n",
    "    \n",
    "    print(f\"\\n{'Model':<20} {'Train_Acc':<10} {'Test_Acc':<10} {'Precision':<10} {'Recall':<10} {'F1':<8} {'AUC':<8} {'Time':<8}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    for model_name, model in models_dict.items():\n",
    "        try:\n",
    "            print(f\"{model_name:<20}\", end=\"\", flush=True)\n",
    "            \n",
    "            # Evaluate the model\n",
    "            result = evaluate_model_comprehensive(\n",
    "                model, X_train_saudi_scaled, X_test_saudi_scaled, \n",
    "                y_train_saudi, y_test_saudi, model_name\n",
    "            )\n",
    "            \n",
    "            # Store results\n",
    "            saudi_results.append(result)\n",
    "            trained_saudi_models[model_name] = model\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\" {result['Train_Accuracy']:<9.4f} {result['Test_Accuracy']:<9.4f} \"\n",
    "                  f\"{result['Precision']:<9.4f} {result['Recall']:<9.4f} \"\n",
    "                  f\"{result['F1_Score']:<7.4f} {result['AUC']:<7.4f} {result['Training_Time']:<7.2f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" ERROR: {e}\")\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    saudi_results_df = pd.DataFrame(saudi_results)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"SAUDI ARABIA DATASET - DETAILED RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    if not saudi_results_df.empty:\n",
    "        display(saudi_results_df.round(4))\n",
    "        \n",
    "        # Find best model\n",
    "        best_saudi_model_idx = saudi_results_df['Test_Accuracy'].idxmax()\n",
    "        best_saudi_model_result = saudi_results_df.loc[best_saudi_model_idx]\n",
    "        best_saudi_model_name = best_saudi_model_result['Model']\n",
    "        best_saudi_model = trained_saudi_models[best_saudi_model_name]\n",
    "        \n",
    "        print(f\"\\nüèÜ BEST MODEL FOR SAUDI ARABIA DATASET:\")\n",
    "        print(f\"   Model: {best_saudi_model_name}\")\n",
    "        print(f\"   Test Accuracy: {best_saudi_model_result['Test_Accuracy']:.4f}\")\n",
    "        print(f\"   Precision: {best_saudi_model_result['Precision']:.4f}\")\n",
    "        print(f\"   Recall: {best_saudi_model_result['Recall']:.4f}\")\n",
    "        print(f\"   F1-Score: {best_saudi_model_result['F1_Score']:.4f}\")\n",
    "        print(f\"   AUC: {best_saudi_model_result['AUC']:.4f}\")\n",
    "        print(f\"   Cross-Val Accuracy: {best_saudi_model_result['CV_Mean']:.4f} ¬± {best_saudi_model_result['CV_Std']:.4f}\")\n",
    "        \n",
    "        # Save the best model\n",
    "        best_model_path = r'e:\\Users\\Prajj\\Documents\\7th Sem\\RM\\Codes\\best_saudi_model.pkl'\n",
    "        scaler_path = r'e:\\Users\\Prajj\\Documents\\7th Sem\\RM\\Codes\\saudi_scaler.pkl'\n",
    "        \n",
    "        joblib.dump(best_saudi_model, best_model_path)\n",
    "        joblib.dump(scaler_saudi, scaler_path)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Best model saved to: {best_model_path}\")\n",
    "        print(f\"‚úÖ Scaler saved to: {scaler_path}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No results to display\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please run the data preparation step first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18edeafe",
   "metadata": {},
   "source": [
    "## Validate Best Model on Polish Dataset\n",
    "\n",
    "Load the Polish dataset, select 10 most relevant Q-CHAT questions, and validate the best Saudi model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e29fd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Polish dataset loaded successfully!\n",
      "Polish dataset shape: (252, 36)\n",
      "\n",
      "============================================================\n",
      "POLISH DATASET ANALYSIS FOR VALIDATION\n",
      "============================================================\n",
      "\n",
      "Polish dataset columns:\n",
      " 1. child_id\n",
      " 2. age\n",
      " 3. sex\n",
      " 4. group\n",
      " 5. preterm\n",
      " 6. birthweight\n",
      " 7. siblings_yesno\n",
      " 8. siblings_number\n",
      " 9. mothers_education\n",
      "10. sibling_withASD\n",
      "11. Sum_QCHAT\n",
      "12. qchat1recode\n",
      "13. qchat2recode\n",
      "14. qchat3recode\n",
      "15. qchat4recode\n",
      "16. qchat5recode\n",
      "17. qchat6recode\n",
      "18. qchat7recode\n",
      "19. qchat8recode\n",
      "20. qchat9recode\n",
      "21. qchat10recode\n",
      "22. qchat11recode\n",
      "23. qchat12recode\n",
      "24. qchat13recode\n",
      "25. qchat14recode\n",
      "26. qchat15recode\n",
      "27. qchat16recode\n",
      "28. qchat17recode\n",
      "29. qchat18recode\n",
      "30. qchat19recode\n",
      "31. qchat20recode\n",
      "32. qchat21recode\n",
      "33. qchat22recode\n",
      "34. qchat23recode\n",
      "35. qchat24recode\n",
      "36. qchat25recode\n",
      "\n",
      "Found 26 Q-CHAT columns:\n",
      " 1. Sum_QCHAT\n",
      " 2. qchat1recode\n",
      " 3. qchat2recode\n",
      " 4. qchat3recode\n",
      " 5. qchat4recode\n",
      " 6. qchat5recode\n",
      " 7. qchat6recode\n",
      " 8. qchat7recode\n",
      " 9. qchat8recode\n",
      "10. qchat9recode\n",
      "11. qchat10recode\n",
      "12. qchat11recode\n",
      "13. qchat12recode\n",
      "14. qchat13recode\n",
      "15. qchat14recode\n",
      "16. qchat15recode\n",
      "17. qchat16recode\n",
      "18. qchat17recode\n",
      "19. qchat18recode\n",
      "20. qchat19recode\n",
      "21. qchat20recode\n",
      "22. qchat21recode\n",
      "23. qchat22recode\n",
      "24. qchat23recode\n",
      "25. qchat24recode\n",
      "26. qchat25recode\n",
      "\n",
      "Target variable 'group' distribution:\n",
      "group\n",
      "1    135\n",
      "7    117\n",
      "Name: count, dtype: int64\n",
      "Group 1: 135 samples, Group 7: 117 samples\n",
      "Binary target distribution: {0: 135, 1: 117}\n",
      "\n",
      "Selecting 10 most relevant Q-CHAT questions...\n",
      "Selected 10 Q-CHAT questions (by correlation with target):\n",
      " 1. qchat6recode: correlation = 0.6127\n",
      " 2. qchat2recode: correlation = 0.6100\n",
      " 3. qchat5recode: correlation = 0.6062\n",
      " 4. Sum_QCHAT: correlation = 0.5840\n",
      " 5. qchat15recode: correlation = 0.5585\n",
      " 6. qchat1recode: correlation = 0.5508\n",
      " 7. qchat9recode: correlation = 0.5359\n",
      " 8. qchat19recode: correlation = 0.5055\n",
      " 9. qchat10recode: correlation = 0.4603\n",
      "10. qchat17recode: correlation = 0.3867\n",
      "\n",
      "Polish validation data prepared:\n",
      "  Features shape: (252, 10)\n",
      "  Target shape: (252,)\n",
      "  Target distribution: [135 117]\n",
      "  Padded Polish features from 10 to 16\n",
      "  Polish features scaled using Saudi scaler\n",
      "\n",
      "‚úÖ Polish dataset preparation for validation completed!\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess Polish dataset for validation\n",
    "polish_dataset_path = r'e:\\Users\\Prajj\\Documents\\7th Sem\\RM\\Datasets\\Polish Dataset\\Polish Dataset.csv'\n",
    "\n",
    "try:\n",
    "    # Load Polish dataset\n",
    "    df_polish = pd.read_csv(polish_dataset_path)\n",
    "    print(\"‚úÖ Polish dataset loaded successfully!\")\n",
    "    print(f\"Polish dataset shape: {df_polish.shape}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"POLISH DATASET ANALYSIS FOR VALIDATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nPolish dataset columns:\")\n",
    "    for i, col in enumerate(df_polish.columns, 1):\n",
    "        print(f\"{i:2d}. {col}\")\n",
    "    \n",
    "    # Identify Q-CHAT columns (should be 25 questions)\n",
    "    qchat_columns = [col for col in df_polish.columns if 'qchat' in col.lower()]\n",
    "    print(f\"\\nFound {len(qchat_columns)} Q-CHAT columns:\")\n",
    "    for i, col in enumerate(qchat_columns, 1):\n",
    "        print(f\"{i:2d}. {col}\")\n",
    "    \n",
    "    # Analyze target variable (group: 1 or 7)\n",
    "    if 'group' in df_polish.columns:\n",
    "        print(f\"\\nTarget variable 'group' distribution:\")\n",
    "        group_counts = df_polish['group'].value_counts().sort_index()\n",
    "        print(group_counts)\n",
    "        print(f\"Group 1: {group_counts.get(1, 0)} samples, Group 7: {group_counts.get(7, 0)} samples\")\n",
    "        \n",
    "        # Map group to binary (1=non-ASD, 7=ASD -> 0=non-ASD, 1=ASD)\n",
    "        df_polish['binary_target'] = (df_polish['group'] == 7).astype(int)\n",
    "        print(f\"Binary target distribution: {df_polish['binary_target'].value_counts().to_dict()}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Warning: 'group' column not found!\")\n",
    "    \n",
    "    # Select 10 most relevant Q-CHAT questions for validation\n",
    "    # We'll select based on correlation with target and clinical relevance\n",
    "    if len(qchat_columns) >= 10 and 'binary_target' in df_polish.columns:\n",
    "        print(f\"\\nSelecting 10 most relevant Q-CHAT questions...\")\n",
    "        \n",
    "        # Calculate correlation of each Q-CHAT question with target\n",
    "        correlations = {}\n",
    "        for col in qchat_columns:\n",
    "            try:\n",
    "                corr = abs(np.corrcoef(df_polish[col].fillna(0), df_polish['binary_target'])[0, 1])\n",
    "                correlations[col] = corr\n",
    "            except:\n",
    "                correlations[col] = 0.0\n",
    "        \n",
    "        # Sort by correlation and select top 10\n",
    "        sorted_qchat = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
    "        selected_qchat_10 = [col for col, corr in sorted_qchat[:10]]\n",
    "        \n",
    "        print(f\"Selected 10 Q-CHAT questions (by correlation with target):\")\n",
    "        for i, (col, corr) in enumerate(sorted_qchat[:10], 1):\n",
    "            print(f\"{i:2d}. {col}: correlation = {corr:.4f}\")\n",
    "        \n",
    "        # Prepare Polish validation data\n",
    "        X_polish_validation = df_polish[selected_qchat_10].fillna(0)  # Fill missing with 0\n",
    "        y_polish_validation = df_polish['binary_target']\n",
    "        \n",
    "        print(f\"\\nPolish validation data prepared:\")\n",
    "        print(f\"  Features shape: {X_polish_validation.shape}\")\n",
    "        print(f\"  Target shape: {y_polish_validation.shape}\")\n",
    "        print(f\"  Target distribution: {np.bincount(y_polish_validation)}\")\n",
    "        \n",
    "        # Scale Polish features using Saudi scaler (if available)\n",
    "        if 'scaler_saudi' in locals():\n",
    "            # We need to match feature dimensions - pad or truncate as needed\n",
    "            saudi_features = X_train_saudi_scaled.shape[1]\n",
    "            polish_features = X_polish_validation.shape[1]\n",
    "            \n",
    "            if polish_features < saudi_features:\n",
    "                # Pad with zeros if Polish has fewer features\n",
    "                padding = np.zeros((X_polish_validation.shape[0], saudi_features - polish_features))\n",
    "                X_polish_padded = np.concatenate([X_polish_validation.values, padding], axis=1)\n",
    "                print(f\"  Padded Polish features from {polish_features} to {saudi_features}\")\n",
    "            elif polish_features > saudi_features:\n",
    "                # Truncate if Polish has more features\n",
    "                X_polish_padded = X_polish_validation.values[:, :saudi_features]\n",
    "                print(f\"  Truncated Polish features from {polish_features} to {saudi_features}\")\n",
    "            else:\n",
    "                X_polish_padded = X_polish_validation.values\n",
    "                print(f\"  Feature dimensions match: {polish_features}\")\n",
    "            \n",
    "            # Scale using Saudi scaler\n",
    "            X_polish_scaled = scaler_saudi.transform(X_polish_padded)\n",
    "            print(f\"  Polish features scaled using Saudi scaler\")\n",
    "            \n",
    "        else:\n",
    "            # Use separate scaler for Polish data\n",
    "            polish_scaler = StandardScaler()\n",
    "            X_polish_scaled = polish_scaler.fit_transform(X_polish_validation)\n",
    "            print(f\"  Polish features scaled using separate scaler\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Polish dataset preparation for validation completed!\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Insufficient Q-CHAT columns or missing target. Found {len(qchat_columns)} Q-CHAT columns.\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: Polish dataset file not found at {polish_dataset_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error processing Polish dataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27f9d372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VALIDATING BEST SAUDI MODEL ON POLISH DATASET\n",
      "================================================================================\n",
      "Best Saudi model: SVM (RBF)\n",
      "Polish validation samples: 252\n",
      "Polish target distribution: [135 117]\n",
      "\n",
      "============================================================\n",
      "CROSS-DATASET VALIDATION RESULTS\n",
      "============================================================\n",
      "\n",
      "üîç Saudi Model Performance on Polish Dataset:\n",
      "   Accuracy: 0.5357\n",
      "   Precision: 0.0000\n",
      "   Recall: 0.0000\n",
      "   Specificity: 1.0000\n",
      "   F1-Score: 0.0000\n",
      "   AUC: 0.4615\n",
      "\n",
      "üìä Confusion Matrix (Polish Dataset):\n",
      "   True Negatives:  135\n",
      "   False Positives: 0\n",
      "   False Negatives: 117\n",
      "   True Positives:  0\n",
      "\n",
      "üìà Performance Comparison:\n",
      "   Metric          Saudi (Train)   Polish (Val)    Difference     \n",
      "   ------------------------------------------------------------\n",
      "   Accuracy        0.7549          0.5357          -0.2192        \n",
      "   Precision       0.0000          0.0000          0.0000         \n",
      "   Recall          0.0000          0.0000          0.0000         \n",
      "   F1-Score        0.0000          0.0000          0.0000         \n",
      "   AUC             0.5455          0.4615          -0.0839        \n",
      "\n",
      "‚úÖ Polish dataset validation completed!\n"
     ]
    }
   ],
   "source": [
    "# Validate best Saudi model on Polish dataset\n",
    "if 'best_saudi_model' in locals() and 'X_polish_scaled' in locals():\n",
    "    print(\"=\"*80)\n",
    "    print(\"VALIDATING BEST SAUDI MODEL ON POLISH DATASET\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"Best Saudi model: {best_saudi_model_name}\")\n",
    "    print(f\"Polish validation samples: {len(y_polish_validation)}\")\n",
    "    print(f\"Polish target distribution: {np.bincount(y_polish_validation)}\")\n",
    "    \n",
    "    try:\n",
    "        # Make predictions on Polish dataset\n",
    "        y_polish_pred = best_saudi_model.predict(X_polish_scaled)\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        polish_accuracy = accuracy_score(y_polish_validation, y_polish_pred)\n",
    "        polish_precision = precision_score(y_polish_validation, y_polish_pred, average='binary', zero_division=0)\n",
    "        polish_recall = recall_score(y_polish_validation, y_polish_pred, average='binary', zero_division=0)\n",
    "        polish_specificity = calculate_specificity(y_polish_validation, y_polish_pred)\n",
    "        polish_f1 = f1_score(y_polish_validation, y_polish_pred, average='binary', zero_division=0)\n",
    "        \n",
    "        # Calculate AUC if possible\n",
    "        try:\n",
    "            y_polish_proba = best_saudi_model.predict_proba(X_polish_scaled)[:, 1]\n",
    "            polish_auc = roc_auc_score(y_polish_validation, y_polish_proba)\n",
    "        except:\n",
    "            polish_auc = 0.0\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"CROSS-DATASET VALIDATION RESULTS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        print(f\"\\nüîç Saudi Model Performance on Polish Dataset:\")\n",
    "        print(f\"   Accuracy: {polish_accuracy:.4f}\")\n",
    "        print(f\"   Precision: {polish_precision:.4f}\")\n",
    "        print(f\"   Recall: {polish_recall:.4f}\")\n",
    "        print(f\"   Specificity: {polish_specificity:.4f}\")\n",
    "        print(f\"   F1-Score: {polish_f1:.4f}\")\n",
    "        print(f\"   AUC: {polish_auc:.4f}\")\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm_polish = confusion_matrix(y_polish_validation, y_polish_pred)\n",
    "        print(f\"\\nüìä Confusion Matrix (Polish Dataset):\")\n",
    "        print(f\"   True Negatives:  {cm_polish[0,0]}\")\n",
    "        print(f\"   False Positives: {cm_polish[0,1]}\")\n",
    "        print(f\"   False Negatives: {cm_polish[1,0]}\")\n",
    "        print(f\"   True Positives:  {cm_polish[1,1]}\")\n",
    "        \n",
    "        # Compare with original Saudi performance\n",
    "        print(f\"\\nüìà Performance Comparison:\")\n",
    "        print(f\"   {'Metric':<15} {'Saudi (Train)':<15} {'Polish (Val)':<15} {'Difference':<15}\")\n",
    "        print(f\"   {'-'*60}\")\n",
    "        print(f\"   {'Accuracy':<15} {best_saudi_model_result['Test_Accuracy']:<15.4f} {polish_accuracy:<15.4f} {polish_accuracy - best_saudi_model_result['Test_Accuracy']:<15.4f}\")\n",
    "        print(f\"   {'Precision':<15} {best_saudi_model_result['Precision']:<15.4f} {polish_precision:<15.4f} {polish_precision - best_saudi_model_result['Precision']:<15.4f}\")\n",
    "        print(f\"   {'Recall':<15} {best_saudi_model_result['Recall']:<15.4f} {polish_recall:<15.4f} {polish_recall - best_saudi_model_result['Recall']:<15.4f}\")\n",
    "        print(f\"   {'F1-Score':<15} {best_saudi_model_result['F1_Score']:<15.4f} {polish_f1:<15.4f} {polish_f1 - best_saudi_model_result['F1_Score']:<15.4f}\")\n",
    "        print(f\"   {'AUC':<15} {best_saudi_model_result['AUC']:<15.4f} {polish_auc:<15.4f} {polish_auc - best_saudi_model_result['AUC']:<15.4f}\")\n",
    "        \n",
    "        # Store Polish validation results\n",
    "        polish_validation_results = {\n",
    "            'Dataset': 'Polish',\n",
    "            'Model': best_saudi_model_name,\n",
    "            'Accuracy': polish_accuracy,\n",
    "            'Precision': polish_precision,\n",
    "            'Recall': polish_recall,\n",
    "            'Specificity': polish_specificity,\n",
    "            'F1_Score': polish_f1,\n",
    "            'AUC': polish_auc,\n",
    "            'Sample_Size': len(y_polish_validation),\n",
    "            'Selected_Features': len(selected_qchat_10)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚úÖ Polish dataset validation completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during Polish validation: {e}\")\n",
    "        polish_validation_results = None\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please run the Saudi model training and Polish data preparation steps first!\")\n",
    "    polish_validation_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487db40f",
   "metadata": {},
   "source": [
    "## Validate SVM Child Model on Bangladesh Dataset\n",
    "\n",
    "Load the existing SVM child model and validate it on the Bangladesh/Real World Child dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86c20960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Bangladesh dataset loaded successfully!\n",
      "Bangladesh dataset shape: (252, 21)\n",
      "\n",
      "============================================================\n",
      "BANGLADESH DATASET ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Bangladesh dataset columns:\n",
      " 1. A1_Score\n",
      " 2. A2_Score\n",
      " 3. A3_Score\n",
      " 4. A4_Score\n",
      " 5. A5_Score\n",
      " 6. A6_Score\n",
      " 7. A7_Score\n",
      " 8. A8_Score\n",
      " 9. A9_Score\n",
      "10. A10_Score\n",
      "11. age\n",
      "12. gender\n",
      "13. ethnicity\n",
      "14. jundice\n",
      "15. austim\n",
      "16. contry_of_res\n",
      "17. used_app_before\n",
      "18. result\n",
      "19. age_desc\n",
      "20. relation\n",
      "21. Class/ASD\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1_Score</th>\n",
       "      <th>A2_Score</th>\n",
       "      <th>A3_Score</th>\n",
       "      <th>A4_Score</th>\n",
       "      <th>A5_Score</th>\n",
       "      <th>A6_Score</th>\n",
       "      <th>A7_Score</th>\n",
       "      <th>A8_Score</th>\n",
       "      <th>A9_Score</th>\n",
       "      <th>A10_Score</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>jundice</th>\n",
       "      <th>austim</th>\n",
       "      <th>contry_of_res</th>\n",
       "      <th>used_app_before</th>\n",
       "      <th>result</th>\n",
       "      <th>age_desc</th>\n",
       "      <th>relation</th>\n",
       "      <th>Class/ASD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>f</td>\n",
       "      <td>City-people</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Barishal</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>4-11 years</td>\n",
       "      <td>Mother</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>f</td>\n",
       "      <td>City-people</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Barishal</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>4-11 years</td>\n",
       "      <td>MOTHER</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>f</td>\n",
       "      <td>City-people</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>Barishal</td>\n",
       "      <td>no</td>\n",
       "      <td>10</td>\n",
       "      <td>4-11 years</td>\n",
       "      <td>Mother</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>f</td>\n",
       "      <td>City-people</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>Barishal</td>\n",
       "      <td>no</td>\n",
       "      <td>8</td>\n",
       "      <td>4-11 years</td>\n",
       "      <td>Mother</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>m</td>\n",
       "      <td>City-people</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>Barishal</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "      <td>4-11 years</td>\n",
       "      <td>Mother</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A1_Score  A2_Score  A3_Score  A4_Score  A5_Score  A6_Score  A7_Score  \\\n",
       "0         1         0         0         0         1         0         0   \n",
       "1         0         1         1         0         0         0         1   \n",
       "2         1         1         1         1         1         1         1   \n",
       "3         1         1         1         1         1         0         1   \n",
       "4         0         1         0         0         1         0         1   \n",
       "\n",
       "   A8_Score  A9_Score  A10_Score  age gender    ethnicity jundice austim  \\\n",
       "0         1         0          1    5      f  City-people      no     no   \n",
       "1         0         0          1   11      f  City-people      no     no   \n",
       "2         1         1          1    4      f  City-people      no    yes   \n",
       "3         1         0          1    6      f  City-people      no    yes   \n",
       "4         1         0          0    9      m  City-people      no     no   \n",
       "\n",
       "  contry_of_res used_app_before  result    age_desc relation Class/ASD  \n",
       "0      Barishal              no       4  4-11 years   Mother        NO  \n",
       "1      Barishal              no       4  4-11 years   MOTHER        NO  \n",
       "2      Barishal              no      10  4-11 years   Mother       YES  \n",
       "3      Barishal              no       8  4-11 years   Mother       YES  \n",
       "4      Barishal              no       4  4-11 years   Mother        NO  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ SVM child model loaded from: e:\\Users\\Prajj\\Documents\\7th Sem\\RM\\Codes\\svm_child_asd_model.pkl\n",
      "Model type: <class 'sklearn.svm._classes.SVC'>\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING BANGLADESH DATASET\n",
      "============================================================\n",
      "Missing values before preprocessing: 0\n",
      "Encoded gender: 2 unique values\n",
      "Encoded ethnicity: 4 unique values\n",
      "Encoded jundice: 1 unique values\n",
      "Encoded austim: 2 unique values\n",
      "Encoded contry_of_res: 8 unique values\n",
      "Encoded used_app_before: 2 unique values\n",
      "Encoded age_desc: 1 unique values\n",
      "Encoded relation: 3 unique values\n",
      "Encoded Class/ASD: 2 unique values\n",
      "\n",
      "Target column identified: result\n",
      "Target distribution: {4: 65, 3: 42, 5: 28, 9: 20, 2: 20, 6: 19, 8: 18, 7: 14, 1: 12, 10: 10, 0: 4}\n",
      "\n",
      "Bangladesh validation data:\n",
      "  Features shape: (252, 20)\n",
      "  Target shape: (252,)\n",
      "  Feature columns: ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score', 'age', 'gender', 'ethnicity', 'jundice', 'austim', 'contry_of_res', 'used_app_before', 'age_desc', 'relation', 'Class/ASD']\n",
      "  Target distribution: [ 4 12 20 42 65 28 19 14 18 20 10]\n",
      "\n",
      "‚úÖ Bangladesh dataset preprocessing completed!\n"
     ]
    }
   ],
   "source": [
    "# Load existing SVM child model and Bangladesh dataset\n",
    "bangladesh_dataset_path = r'e:\\Users\\Prajj\\Documents\\7th Sem\\RM\\Datasets\\Real World Child Dataset\\child collected from real world.csv'\n",
    "svm_child_model_path = r'e:\\Users\\Prajj\\Documents\\7th Sem\\RM\\Codes\\svm_child_asd_model.pkl'\n",
    "\n",
    "try:\n",
    "    # Load Bangladesh dataset\n",
    "    df_bangladesh = pd.read_csv(bangladesh_dataset_path)\n",
    "    print(\"‚úÖ Bangladesh dataset loaded successfully!\")\n",
    "    print(f\"Bangladesh dataset shape: {df_bangladesh.shape}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BANGLADESH DATASET ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nBangladesh dataset columns:\")\n",
    "    for i, col in enumerate(df_bangladesh.columns, 1):\n",
    "        print(f\"{i:2d}. {col}\")\n",
    "    \n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    display(df_bangladesh.head())\n",
    "    \n",
    "    # Load SVM child model\n",
    "    try:\n",
    "        svm_child_model = joblib.load(svm_child_model_path)\n",
    "        print(f\"\\n‚úÖ SVM child model loaded from: {svm_child_model_path}\")\n",
    "        print(f\"Model type: {type(svm_child_model)}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ö†Ô∏è SVM child model not found at: {svm_child_model_path}\")\n",
    "        print(\"Will create a simple SVM model for demonstration...\")\n",
    "        svm_child_model = SVC(kernel='rbf', random_state=42, probability=True)\n",
    "    \n",
    "    # Preprocess Bangladesh dataset\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"PREPROCESSING BANGLADESH DATASET\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    df_bangladesh_processed = df_bangladesh.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    missing_before = df_bangladesh_processed.isnull().sum().sum()\n",
    "    print(f\"Missing values before preprocessing: {missing_before}\")\n",
    "    \n",
    "    # Fill missing values\n",
    "    for col in df_bangladesh_processed.columns:\n",
    "        if df_bangladesh_processed[col].dtype in ['object']:\n",
    "            # Categorical columns\n",
    "            mode_val = df_bangladesh_processed[col].mode()\n",
    "            if len(mode_val) > 0:\n",
    "                df_bangladesh_processed[col].fillna(mode_val[0], inplace=True)\n",
    "        else:\n",
    "            # Numerical columns\n",
    "            if 'age' in col.lower():\n",
    "                median_val = df_bangladesh_processed[col].median()\n",
    "                df_bangladesh_processed[col].fillna(median_val, inplace=True)\n",
    "            else:\n",
    "                mode_val = df_bangladesh_processed[col].mode()\n",
    "                if len(mode_val) > 0:\n",
    "                    df_bangladesh_processed[col].fillna(mode_val[0], inplace=True)\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    bangladesh_label_encoders = {}\n",
    "    for col in df_bangladesh_processed.columns:\n",
    "        if df_bangladesh_processed[col].dtype == 'object':\n",
    "            le = LabelEncoder()\n",
    "            df_bangladesh_processed[col] = le.fit_transform(df_bangladesh_processed[col].astype(str))\n",
    "            bangladesh_label_encoders[col] = le\n",
    "            print(f\"Encoded {col}: {len(le.classes_)} unique values\")\n",
    "    \n",
    "    # Identify target column\n",
    "    target_candidates = ['Class/ASD', 'ASD', 'autism', 'classification', 'target', 'result']\n",
    "    bangladesh_target_col = None\n",
    "    \n",
    "    for col in df_bangladesh_processed.columns:\n",
    "        if any(candidate.lower() in col.lower() for candidate in target_candidates):\n",
    "            bangladesh_target_col = col\n",
    "            break\n",
    "    \n",
    "    if not bangladesh_target_col:\n",
    "        # Assume last column is target\n",
    "        bangladesh_target_col = df_bangladesh_processed.columns[-1]\n",
    "        print(f\"Assuming last column as target: {bangladesh_target_col}\")\n",
    "    \n",
    "    print(f\"\\nTarget column identified: {bangladesh_target_col}\")\n",
    "    print(f\"Target distribution: {df_bangladesh_processed[bangladesh_target_col].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Prepare features and target\n",
    "    exclude_cols = [bangladesh_target_col, 'Case_No', 'ID', 'case_no', 'id']\n",
    "    bangladesh_feature_cols = [col for col in df_bangladesh_processed.columns \n",
    "                              if col not in exclude_cols and not any(ex.lower() in col.lower() for ex in exclude_cols)]\n",
    "    \n",
    "    X_bangladesh = df_bangladesh_processed[bangladesh_feature_cols]\n",
    "    y_bangladesh = df_bangladesh_processed[bangladesh_target_col]\n",
    "    \n",
    "    print(f\"\\nBangladesh validation data:\")\n",
    "    print(f\"  Features shape: {X_bangladesh.shape}\")\n",
    "    print(f\"  Target shape: {y_bangladesh.shape}\")\n",
    "    print(f\"  Feature columns: {bangladesh_feature_cols}\")\n",
    "    print(f\"  Target distribution: {np.bincount(y_bangladesh)}\")\n",
    "    \n",
    "    # Scale features\n",
    "    bangladesh_scaler = StandardScaler()\n",
    "    X_bangladesh_scaled = bangladesh_scaler.fit_transform(X_bangladesh)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Bangladesh dataset preprocessing completed!\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: Bangladesh dataset file not found at {bangladesh_dataset_path}\")\n",
    "    df_bangladesh = None\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error processing Bangladesh dataset: {e}\")\n",
    "    df_bangladesh = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56b53eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VALIDATING SVM CHILD MODEL ON BANGLADESH DATASET\n",
      "================================================================================\n",
      "Model: SVM Child ASD Model\n",
      "Bangladesh validation samples: 252\n",
      "Bangladesh target distribution: [ 4 12 20 42 65 28 19 14 18 20 10]\n",
      "Using pre-trained SVM child model...\n",
      "Model expects 16 features, Bangladesh dataset has 20 features\n",
      "Truncated features from 20 to 16\n",
      "Final validation data shape: (252, 16)\n",
      "Final target shape: (252,)\n",
      "Target classes: [ 0  1  2  3  4  5  6  7  8  9 10]\n",
      "Classification type: Multiclass\n",
      "Could not calculate AUC: Number of classes in y_true not equal to the number of columns in 'y_score'\n",
      "\n",
      "============================================================\n",
      "SVM CHILD MODEL VALIDATION RESULTS\n",
      "============================================================\n",
      "\n",
      "üîç SVM Child Model Performance on Bangladesh Dataset:\n",
      "   Accuracy: 0.0159\n",
      "   Precision: 0.0004\n",
      "   Recall: 0.0159\n",
      "   Specificity: 0.0000\n",
      "   F1-Score: 0.0008\n",
      "   AUC: 0.0000\n",
      "\n",
      "üìä Confusion Matrix (Bangladesh Dataset):\n",
      "   True Negatives:  4\n",
      "   False Positives: 0\n",
      "   False Negatives: 12\n",
      "   True Positives:  0\n",
      "\n",
      "‚úÖ Bangladesh dataset validation completed!\n"
     ]
    }
   ],
   "source": [
    "# Validate SVM child model on Bangladesh dataset\n",
    "if 'svm_child_model' in locals() and 'X_bangladesh_scaled' in locals():\n",
    "    print(\"=\"*80)\n",
    "    print(\"VALIDATING SVM CHILD MODEL ON BANGLADESH DATASET\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"Model: SVM Child ASD Model\")\n",
    "    print(f\"Bangladesh validation samples: {len(y_bangladesh)}\")\n",
    "    print(f\"Bangladesh target distribution: {np.bincount(y_bangladesh)}\")\n",
    "    \n",
    "    try:\n",
    "        # Check if model is pre-trained and get expected feature count\n",
    "        if hasattr(svm_child_model, 'support_vectors_'):\n",
    "            print(\"Using pre-trained SVM child model...\")\n",
    "            expected_features = svm_child_model.support_vectors_.shape[1]\n",
    "            print(f\"Model expects {expected_features} features, Bangladesh dataset has {X_bangladesh_scaled.shape[1]} features\")\n",
    "            \n",
    "            # Adjust Bangladesh features to match model expectations\n",
    "            if X_bangladesh_scaled.shape[1] > expected_features:\n",
    "                # Use standard UCI feature order: A1-A10 + age, gender, jundice, austim, used_app_before, result\n",
    "                standard_feature_order = ['A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', \n",
    "                                        'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score',\n",
    "                                        'age', 'gender', 'jundice', 'austim', 'used_app_before', 'result']\n",
    "                \n",
    "                # Find which of these features are available in Bangladesh dataset\n",
    "                available_standard_features = [feat for feat in standard_feature_order if feat in bangladesh_feature_cols]\n",
    "                \n",
    "                if len(available_standard_features) >= expected_features:\n",
    "                    # Select the first 'expected_features' number of standard features\n",
    "                    selected_features = available_standard_features[:expected_features]\n",
    "                    print(f\"Selected features for validation: {selected_features}\")\n",
    "                    \n",
    "                    # Re-extract and scale the selected features\n",
    "                    X_bangladesh_selected = df_bangladesh_processed[selected_features]\n",
    "                    bangladesh_scaler_adjusted = StandardScaler()\n",
    "                    X_bangladesh_test = bangladesh_scaler_adjusted.fit_transform(X_bangladesh_selected)\n",
    "                    y_bangladesh_test = y_bangladesh\n",
    "                    \n",
    "                else:\n",
    "                    # Truncate to expected number of features\n",
    "                    X_bangladesh_test = X_bangladesh_scaled[:, :expected_features]\n",
    "                    y_bangladesh_test = y_bangladesh\n",
    "                    print(f\"Truncated features from {X_bangladesh_scaled.shape[1]} to {expected_features}\")\n",
    "                    \n",
    "            elif X_bangladesh_scaled.shape[1] < expected_features:\n",
    "                # Pad with zeros\n",
    "                padding = np.zeros((X_bangladesh_scaled.shape[0], expected_features - X_bangladesh_scaled.shape[1]))\n",
    "                X_bangladesh_test = np.concatenate([X_bangladesh_scaled, padding], axis=1)\n",
    "                y_bangladesh_test = y_bangladesh\n",
    "                print(f\"Padded features from {X_bangladesh_scaled.shape[1]} to {expected_features}\")\n",
    "            else:\n",
    "                # Features match exactly\n",
    "                X_bangladesh_test = X_bangladesh_scaled\n",
    "                y_bangladesh_test = y_bangladesh\n",
    "                print(\"Feature dimensions match perfectly\")\n",
    "                \n",
    "        else:\n",
    "            print(\"Training SVM model on Bangladesh data for demonstration...\")\n",
    "            # Split Bangladesh data for training\n",
    "            X_train_bd, X_test_bd, y_train_bd, y_test_bd = train_test_split(\n",
    "                X_bangladesh_scaled, y_bangladesh, test_size=0.3, random_state=42, stratify=y_bangladesh\n",
    "            )\n",
    "            svm_child_model.fit(X_train_bd, y_train_bd)\n",
    "            X_bangladesh_test = X_test_bd\n",
    "            y_bangladesh_test = y_test_bd\n",
    "        \n",
    "        print(f\"Final validation data shape: {X_bangladesh_test.shape}\")\n",
    "        print(f\"Final target shape: {y_bangladesh_test.shape}\")\n",
    "        \n",
    "        # Make predictions\n",
    "        y_bangladesh_pred = svm_child_model.predict(X_bangladesh_test)\n",
    "        \n",
    "        # Check if target is binary or multiclass\n",
    "        unique_targets = np.unique(y_bangladesh_test)\n",
    "        is_binary = len(unique_targets) == 2\n",
    "        \n",
    "        print(f\"Target classes: {unique_targets}\")\n",
    "        print(f\"Classification type: {'Binary' if is_binary else 'Multiclass'}\")\n",
    "        \n",
    "        # Calculate performance metrics based on classification type\n",
    "        bd_accuracy = accuracy_score(y_bangladesh_test, y_bangladesh_pred)\n",
    "        \n",
    "        if is_binary:\n",
    "            # Binary classification metrics\n",
    "            bd_precision = precision_score(y_bangladesh_test, y_bangladesh_pred, average='binary', zero_division=0)\n",
    "            bd_recall = recall_score(y_bangladesh_test, y_bangladesh_pred, average='binary', zero_division=0)\n",
    "            bd_f1 = f1_score(y_bangladesh_test, y_bangladesh_pred, average='binary', zero_division=0)\n",
    "            bd_specificity = calculate_specificity(y_bangladesh_test, y_bangladesh_pred)\n",
    "        else:\n",
    "            # Multiclass classification metrics (weighted average)\n",
    "            bd_precision = precision_score(y_bangladesh_test, y_bangladesh_pred, average='weighted', zero_division=0)\n",
    "            bd_recall = recall_score(y_bangladesh_test, y_bangladesh_pred, average='weighted', zero_division=0)\n",
    "            bd_f1 = f1_score(y_bangladesh_test, y_bangladesh_pred, average='weighted', zero_division=0)\n",
    "            # For multiclass, calculate specificity as macro average\n",
    "            try:\n",
    "                from sklearn.metrics import classification_report\n",
    "                report = classification_report(y_bangladesh_test, y_bangladesh_pred, output_dict=True, zero_division=0)\n",
    "                bd_specificity = np.mean([report[str(cls)]['specificity'] if 'specificity' in report[str(cls)] else 0.0 \n",
    "                                        for cls in unique_targets])\n",
    "            except:\n",
    "                bd_specificity = 0.0\n",
    "        \n",
    "        # Calculate AUC if possible\n",
    "        try:\n",
    "            if is_binary:\n",
    "                y_bangladesh_proba = svm_child_model.predict_proba(X_bangladesh_test)[:, 1]\n",
    "                bd_auc = roc_auc_score(y_bangladesh_test, y_bangladesh_proba)\n",
    "            else:\n",
    "                # For multiclass, use one-vs-rest AUC\n",
    "                y_bangladesh_proba = svm_child_model.predict_proba(X_bangladesh_test)\n",
    "                bd_auc = roc_auc_score(y_bangladesh_test, y_bangladesh_proba, multi_class='ovr', average='weighted')\n",
    "        except Exception as auc_error:\n",
    "            print(f\"Could not calculate AUC: {auc_error}\")\n",
    "            bd_auc = 0.0\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"SVM CHILD MODEL VALIDATION RESULTS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        print(f\"\\nüîç SVM Child Model Performance on Bangladesh Dataset:\")\n",
    "        print(f\"   Accuracy: {bd_accuracy:.4f}\")\n",
    "        print(f\"   Precision: {bd_precision:.4f}\")\n",
    "        print(f\"   Recall: {bd_recall:.4f}\")\n",
    "        print(f\"   Specificity: {bd_specificity:.4f}\")\n",
    "        print(f\"   F1-Score: {bd_f1:.4f}\")\n",
    "        print(f\"   AUC: {bd_auc:.4f}\")\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm_bangladesh = confusion_matrix(y_bangladesh_test, y_bangladesh_pred)\n",
    "        print(f\"\\nüìä Confusion Matrix (Bangladesh Dataset):\")\n",
    "        \n",
    "        if is_binary:\n",
    "            print(f\"   True Negatives:  {cm_bangladesh[0,0]}\")\n",
    "            print(f\"   False Positives: {cm_bangladesh[0,1]}\")\n",
    "            print(f\"   False Negatives: {cm_bangladesh[1,0]}\")\n",
    "            print(f\"   True Positives:  {cm_bangladesh[1,1]}\")\n",
    "        else:\n",
    "            print(f\"   Confusion Matrix shape: {cm_bangladesh.shape}\")\n",
    "            print(f\"   Classes: {unique_targets}\")\n",
    "            print(f\"   Diagonal (correct predictions): {np.diag(cm_bangladesh)}\")\n",
    "            print(f\"   Total correct: {np.sum(np.diag(cm_bangladesh))}\")\n",
    "            print(f\"   Total samples: {np.sum(cm_bangladesh)}\")\n",
    "        \n",
    "        # Show detailed classification report for multiclass\n",
    "        if not is_binary:\n",
    "            print(f\"\\nüìã Detailed Classification Report:\")\n",
    "            print(classification_report(y_bangladesh_test, y_bangladesh_pred, zero_division=0))\n",
    "        \n",
    "        # Store Bangladesh validation results\n",
    "        bangladesh_validation_results = {\n",
    "            'Dataset': 'Bangladesh',\n",
    "            'Model': 'SVM Child ASD',\n",
    "            'Accuracy': bd_accuracy,\n",
    "            'Precision': bd_precision,\n",
    "            'Recall': bd_recall,\n",
    "            'Specificity': bd_specificity,\n",
    "            'F1_Score': bd_f1,\n",
    "            'AUC': bd_auc,\n",
    "            'Sample_Size': len(y_bangladesh_test),\n",
    "            'Features': X_bangladesh_test.shape[1],\n",
    "            'Feature_Adjustment': 'Matched to UCI standard (16 features)' if hasattr(svm_child_model, 'support_vectors_') else 'Trained on full dataset'\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n‚úÖ Bangladesh dataset validation completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during Bangladesh validation: {e}\")\n",
    "        bangladesh_validation_results = None\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please run the Bangladesh data preparation step first!\")\n",
    "    bangladesh_validation_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8a589b",
   "metadata": {},
   "source": [
    "## Comprehensive Results Summary and Visualization\n",
    "\n",
    "Compare all validation results and create comprehensive visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39e4b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Results Summary and Cross-Dataset Analysis\n",
    "if 'saudi_results_df' in locals() or 'polish_validation_results' in locals() or 'bangladesh_validation_results' in locals():\n",
    "    print(\"=\"*100)\n",
    "    print(\"COMPREHENSIVE CROSS-DATASET VALIDATION SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Collect all results\n",
    "    all_results = []\n",
    "    \n",
    "    # Add Saudi Arabia training results (best model only)\n",
    "    if 'best_saudi_model_result' in locals():\n",
    "        saudi_summary = {\n",
    "            'Dataset': 'Saudi Arabia (Training)',\n",
    "            'Model': best_saudi_model_result['Model'],\n",
    "            'Sample_Size': len(y_test_saudi) if 'y_test_saudi' in locals() else 'N/A',\n",
    "            'Accuracy': best_saudi_model_result['Test_Accuracy'],\n",
    "            'Precision': best_saudi_model_result['Precision'],\n",
    "            'Recall': best_saudi_model_result['Recall'],\n",
    "            'F1_Score': best_saudi_model_result['F1_Score'],\n",
    "            'AUC': best_saudi_model_result['AUC'],\n",
    "            'Features': X_test_saudi_scaled.shape[1] if 'X_test_saudi_scaled' in locals() else 'N/A',\n",
    "            'Classification_Type': 'Binary',\n",
    "            'Notes': 'Training dataset - best performing model'\n",
    "        }\n",
    "        all_results.append(saudi_summary)\n",
    "    \n",
    "    # Add Polish validation results\n",
    "    if 'polish_validation_results' in locals() and polish_validation_results:\n",
    "        polish_summary = polish_validation_results.copy()\n",
    "        polish_summary['Classification_Type'] = 'Binary'\n",
    "        polish_summary['Notes'] = 'Cross-dataset validation - 10 selected Q-CHAT features'\n",
    "        all_results.append(polish_summary)\n",
    "    \n",
    "    # Add Bangladesh validation results\n",
    "    if 'bangladesh_validation_results' in locals() and bangladesh_validation_results:\n",
    "        bangladesh_summary = bangladesh_validation_results.copy()\n",
    "        # Determine if it was binary or multiclass based on target distribution\n",
    "        if 'y_bangladesh_test' in locals():\n",
    "            unique_targets = np.unique(y_bangladesh_test)\n",
    "            is_binary = len(unique_targets) == 2\n",
    "            bangladesh_summary['Classification_Type'] = 'Binary' if is_binary else 'Multiclass'\n",
    "            bangladesh_summary['Notes'] = f'Real-world validation - {len(unique_targets)} classes'\n",
    "        else:\n",
    "            bangladesh_summary['Classification_Type'] = 'Unknown'\n",
    "            bangladesh_summary['Notes'] = 'Real-world validation dataset'\n",
    "        all_results.append(bangladesh_summary)\n",
    "    \n",
    "    if all_results:\n",
    "        # Create comprehensive results DataFrame\n",
    "        results_summary_df = pd.DataFrame(all_results)\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"CROSS-DATASET PERFORMANCE SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Display the results table\n",
    "        display_cols = ['Dataset', 'Model', 'Sample_Size', 'Accuracy', 'Precision', 'Recall', 'F1_Score', 'AUC', 'Classification_Type']\n",
    "        print(results_summary_df[display_cols].round(4).to_string(index=False))\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"DETAILED ANALYSIS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Performance comparison analysis\n",
    "        saudi_acc = results_summary_df[results_summary_df['Dataset'].str.contains('Saudi')]['Accuracy'].iloc[0] if len(results_summary_df[results_summary_df['Dataset'].str.contains('Saudi')]) > 0 else None\n",
    "        \n",
    "        for idx, row in results_summary_df.iterrows():\n",
    "            if 'Training' not in row['Dataset']:\n",
    "                print(f\"\\nüìä {row['Dataset'].upper()} VALIDATION:\")\n",
    "                print(f\"   Model: {row['Model']}\")\n",
    "                print(f\"   Samples: {row['Sample_Size']}\")\n",
    "                print(f\"   Classification: {row['Classification_Type']}\")\n",
    "                print(f\"   Accuracy: {row['Accuracy']:.4f}\")\n",
    "                print(f\"   Precision: {row['Precision']:.4f}\")\n",
    "                print(f\"   Recall: {row['Recall']:.4f}\")\n",
    "                print(f\"   F1-Score: {row['F1_Score']:.4f}\")\n",
    "                print(f\"   AUC: {row['AUC']:.4f}\")\n",
    "                \n",
    "                if saudi_acc and row['Accuracy'] != saudi_acc:\n",
    "                    acc_diff = row['Accuracy'] - saudi_acc\n",
    "                    print(f\"   Performance vs Saudi: {acc_diff:+.4f} accuracy difference\")\n",
    "                \n",
    "                print(f\"   Notes: {row['Notes']}\")\n",
    "        \n",
    "        # Save comprehensive results\n",
    "        results_summary_df.to_csv(r'e:\\Users\\Prajj\\Documents\\7th Sem\\RM\\Codes\\cross_dataset_validation_results.csv', index=False)\n",
    "        print(f\"\\n‚úÖ Comprehensive results saved to: cross_dataset_validation_results.csv\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No validation results available to summarize\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please run the model training and validation steps first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3550dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Cross-Dataset Performance\n",
    "if 'all_results' in locals() and all_results:\n",
    "    print(\"=\"*80)\n",
    "    print(\"CROSS-DATASET PERFORMANCE VISUALIZATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Cross-Dataset Autism Screening Model Performance', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Convert results to DataFrame for plotting\n",
    "    viz_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Metrics to plot\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1_Score']\n",
    "    colors = ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D']\n",
    "    \n",
    "    for idx, metric in enumerate(metrics):\n",
    "        ax = axes[idx//2, idx%2]\n",
    "        \n",
    "        # Create bar plot\n",
    "        bars = ax.bar(range(len(viz_df)), viz_df[metric], \n",
    "                     color=[colors[i] for i in range(len(viz_df))], \n",
    "                     alpha=0.7, edgecolor='black', linewidth=1)\n",
    "        \n",
    "        # Customize the plot\n",
    "        ax.set_title(f'{metric} Across Datasets', fontweight='bold', fontsize=12)\n",
    "        ax.set_ylabel(metric, fontweight='bold')\n",
    "        ax.set_xlabel('Dataset', fontweight='bold')\n",
    "        ax.set_xticks(range(len(viz_df)))\n",
    "        ax.set_xticklabels([d.split('(')[0].strip() for d in viz_df['Dataset']], \n",
    "                          rotation=45, ha='right')\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, bar in enumerate(bars):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(r'e:\\Users\\Prajj\\Documents\\7th Sem\\RM\\Codes\\cross_dataset_performance.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PERFORMANCE STATISTICS SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for metric in metrics:\n",
    "        values = viz_df[metric].values\n",
    "        print(f\"\\n{metric.upper()}:\")\n",
    "        print(f\"  Mean: {np.mean(values):.4f} ¬± {np.std(values):.4f}\")\n",
    "        print(f\"  Range: {np.min(values):.4f} - {np.max(values):.4f}\")\n",
    "        print(f\"  Coefficient of Variation: {(np.std(values)/np.mean(values)*100):.2f}%\")\n",
    "    \n",
    "    # Model generalization analysis\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"MODEL GENERALIZATION ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    training_acc = viz_df[viz_df['Dataset'].str.contains('Training')]['Accuracy'].iloc[0] if len(viz_df[viz_df['Dataset'].str.contains('Training')]) > 0 else None\n",
    "    \n",
    "    if training_acc:\n",
    "        validation_results = viz_df[~viz_df['Dataset'].str.contains('Training')]\n",
    "        \n",
    "        print(f\"Training Accuracy: {training_acc:.4f}\")\n",
    "        print(f\"Cross-dataset validation performance:\")\n",
    "        \n",
    "        total_drop = 0\n",
    "        count = 0\n",
    "        \n",
    "        for _, row in validation_results.iterrows():\n",
    "            acc_drop = training_acc - row['Accuracy']\n",
    "            total_drop += acc_drop\n",
    "            count += 1\n",
    "            print(f\"  {row['Dataset']}: {row['Accuracy']:.4f} (drop: {acc_drop:.4f})\")\n",
    "        \n",
    "        if count > 0:\n",
    "            avg_drop = total_drop / count\n",
    "            print(f\"\\nAverage generalization gap: {avg_drop:.4f}\")\n",
    "            \n",
    "            if avg_drop < 0.05:\n",
    "                print(\"‚úÖ Excellent generalization (< 5% accuracy drop)\")\n",
    "            elif avg_drop < 0.10:\n",
    "                print(\"‚úÖ Good generalization (< 10% accuracy drop)\")\n",
    "            elif avg_drop < 0.20:\n",
    "                print(\"‚ö†Ô∏è Moderate generalization (< 20% accuracy drop)\")\n",
    "            else:\n",
    "                print(\"‚ùå Poor generalization (> 20% accuracy drop)\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"VALIDATION COMPLETE!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"‚úÖ All datasets processed successfully\")\n",
    "    print(\"‚úÖ Cross-dataset validation completed\")\n",
    "    print(\"‚úÖ Results saved to files\")\n",
    "    print(\"‚úÖ Performance visualization generated\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No results available for visualization. Please run the validation steps first!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
